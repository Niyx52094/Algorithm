{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append(\"D:\\CS\\MachineLearning\\Dive-into-DL-PyTorch-master\\code\") \n",
    "import d2lzh_pytorch as d2l\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_data_jay_lyrics():\n",
    "    \"\"\"加载周杰伦歌词数据集\"\"\"\n",
    "    with zipfile.ZipFile(r'D:\\CS\\MachineLearning\\Dive-into-DL-PyTorch-master\\data\\jaychou_lyrics.txt.zip') as zin:\n",
    "        with zin.open('jaychou_lyrics.txt') as f:\n",
    "            corpus_chars = f.read().decode('utf-8')\n",
    "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    corpus_chars = corpus_chars[0:10000]\n",
    "    idx_to_char = list(set(corpus_chars))\n",
    "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "    vocab_size = len(char_to_idx)\n",
    "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "    return corpus_indices, char_to_idx, idx_to_char, vocab_size\n",
    "\n",
    "# (corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(corpus_indices, char_to_idx, idx_to_char, vocab_size) =load_data_jay_lyrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "num_hiddens=256\n",
    "#rnn_layer=nn.LSTM(input_size=vocab_size,hidden_size=num_hiddens)\n",
    "rnn_layer=nn.RNN(input_size=vocab_size,hidden_size=num_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 2, 256]) 1 torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "num_steps=35\n",
    "batch_size=2\n",
    "state=None\n",
    "X=torch.rand(num_steps,batch_size,vocab_size)\n",
    "Y,state_new=rnn_layer(X,state)\n",
    "print(Y.shape,len(state_new),state_new[0].shape)#输出形状为（时间步数，批量大小，隐藏单元个数），\n",
    "                                                #隐藏状态的形状为（层数，批量大小，隐藏单元个数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define RNN model with an dense output\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self,rnn_layer,vocab_size):\n",
    "        super(RNNModel,self).__init__()\n",
    "        self.rnn=rnn_layer\n",
    "        self.hidden_size=rnn_layer.hidden_size*(2 if rnn_layer.bidirectional else 1)\n",
    "        self.vocab_size=vocab_size\n",
    "        self.dense=nn.Linear(self.hidden_size,vocab_size)\n",
    "        self.state=None\n",
    "        \n",
    "    def forward(self,inputs,state):\n",
    "        #获取one-hot向量\n",
    "        X=d2l.to_onehot(inputs,self.vocab_size)\n",
    "        Y,self.state=self.rnn(torch.stack(X),state)\n",
    "        #全连接层会将y的形状变成（num_steps*batch_size,num_hiddens）\n",
    "        output=self.dense(Y.view(-1,Y.shape[-1]))\n",
    "        return output,self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict rnn model based on the predix by the largest weight\n",
    "def predict_rnn_pytorch(prefix,num_chars,model,vocab_size,device,idx_to_char,char_to_idx):\n",
    "    state=None\n",
    "    output=[char_to_idx[prefix[0]]]#output 会记录prefix加上输出\n",
    "    for t in range(num_chars+len(prefix)-1):\n",
    "        X=torch.tensor([output[-1]],device=device).view(1,1)\n",
    "        if state is not None:\n",
    "            if isinstance(state,tuple):#LSTM,state：（h,c）\n",
    "                state=(state[0].to(device),state[1].to(device))\n",
    "            else:\n",
    "                state=state.to(device)\n",
    "                \n",
    "        (Y,state)=model(X,state)\n",
    "        if t<len(prefix)-1:\n",
    "            output.append(char_to_idx[prefix[t+1]])\n",
    "        else:\n",
    "            output.append(int(Y.argmax(dim=1).item()))\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'分开渡靠条条条条条条条条'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=RNNModel(rnn_layer,vocab_size).to(device)\n",
    "predict_rnn_pytorch('分开',10,model,vocab_size,device,idx_to_char,char_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "def train_and_predict_rnn_pytorch(model,num_hiddens,vocab_size,device,corpus_indices,idx_to_char,char_to_idx,\n",
    "                                 num_epochs,num_steps,lr,clipping_theta,batch_size,pred_period,pred_len,prefixes):\n",
    "    loss=nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "    model.to(device)\n",
    "    state=None\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum,n,start=0.0,0,time.time()\n",
    "        data_iter=d2l.data_iter_consecutive(corpus_indices,batch_size,num_steps,device)#相邻采样\n",
    "        for X,Y in data_iter:\n",
    "            if state is not None:\n",
    "                #使用detach函数从计算图中分离隐藏态，这是为了\n",
    "                #使得模型参数的梯度计算只依赖一次迭代读取的小批量序列（防止梯度开销太大）\n",
    "                if isinstance(state,tuple):\n",
    "                    state=(state[0].detach(),state[1].detach())\n",
    "                else:\n",
    "                    state=state.detach()\n",
    "            (output,state)=model(X,state)#output 形状为（num_steps*batch_size,vocab_size）\n",
    "            \n",
    "            #Y的形状是(batch_size,num_steps)，转置后便哼长度batch_size*num_steps的向量，跟输出行一一对应\n",
    "            y=torch.transpose(Y,0,1).contiguous().view(-1)\n",
    "            l=loss(output,y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            #梯度裁剪\n",
    "            d2l.grad_clipping(model.parameters(),clipping_theta,device)\n",
    "            optimizer.step()\n",
    "            l_sum+=l.item()*y.shape[0]\n",
    "            n+=y.shape[0]\n",
    "        \n",
    "        try:\n",
    "            perplexity=math.exp(l_sum/n)\n",
    "        except OverflowError:\n",
    "            perplexity=float('inf')\n",
    "        if (epoch+1)% pred_period==0:\n",
    "            print(\"epoch %d, perplexity %f,time %.2f sec\" % (\n",
    "            epoch+1,perplexity,time.time()-start))\n",
    "            \n",
    "            for prefix in prefixes:\n",
    "                print('-',predict_rnn_pytorch(\n",
    "                prefix,pred_len,model,vocab_size,device,idx_to_char,char_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, perplexity 10.396823,time 0.95 sec\n",
      "- 分开始我不 再想 你不的我  你想要你 我不多 我 你不我 你你在我想你 我想你 我不多 别人我  你是\n",
      "- 不分开 我想要你 我想多 我爱你 我想多 我不能 我想你你的爱我 你这样的手不  想要你你 我有了这样我想\n",
      "epoch 100, perplexity 1.297595,time 0.96 sec\n",
      "- 分开 我有人难过 还在空 快谁是神不了 习惯 在吸完血后开始打呼 管家是一只会说法语举止优雅的猪 吸血前\n",
      "- 不分开 我我不能痛  我不 我想你 不要去  回小的外桌椅 一起看着  想带和 看着的河爱 说 别你的世爸\n",
      "epoch 150, perplexity 1.067417,time 0.96 sec\n",
      "- 分开 我有人难过 还已经离你我 我有多难熬  穿过云层 我试著努力向你奔跑 爱才送到 你却已在别人怀抱 \n",
      "- 不分开 了我来不多   没有你在我不能再这样 我们就很我不多 样 我不想你想要你我的手不能 语你可 看你已\n",
      "epoch 200, perplexity 1.034511,time 0.98 sec\n",
      "- 分开 我有人练 你 一壶好酒 再来一碗热粥 配上几斤的牛肉 我说店小二 三两银够不够 景色入秋 漫天黄沙\n",
      "- 不分开 我我不好多  没有没有你说 有是难过  不懂 你 一定有痛 像龙的可爱 我 想这样的甜蜜 所有一双\n",
      "epoch 250, perplexity 1.023160,time 0.98 sec\n",
      "- 分开 我有人练 多 已经猜透 透不想通你 我的脚起 戒指在空屋 在灌木丛旁邂逅 一只令它心仪的母斑鸠 印\n",
      "- 不分开 我我不好多  多说没多 说是我想要你看棒  想要这 我已无处可躲 我不要再想 我不要再想 我不 我\n"
     ]
    }
   ],
   "source": [
    "num_epoches,batch_size,lr,clipping_theta=250,32,1e-3,1e-2\n",
    "pred_period,pred_len,prefixes= 50,50,[\"分开\",\"不分开\"]\n",
    "train_and_predict_rnn_pytorch(model,num_hiddens,vocab_size,device,corpus_indices,idx_to_char,\n",
    "                             char_to_idx,num_epoches,num_steps,lr,clipping_theta,batch_size,pred_period,pred_len,prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
