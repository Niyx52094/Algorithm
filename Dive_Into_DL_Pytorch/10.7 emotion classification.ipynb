{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import random\n",
    "import tarfile\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchtext.vocab as Vocab\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"D:\\CS\\MachineLearning\\Dive-into-DL-PyTorch-master\\code\") \n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATA_ROOT = r\"D:\\CS\\MachineLearning\\Dive-into-DL-PyTorch-master\\S1\\CSCL\\tangss\\Datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use stanford's large movie review dateaset including 25000 reviews in IMDB\n",
    "#download dataset\n",
    "fname = os.path.join(DATA_ROOT, \"aclImdb_v1.tar.gz\")\n",
    "if not os.path.exists(os.path.join(DATA_ROOT, \"aclImdb\")):\n",
    "    print(\"从压缩包解压...\")\n",
    "    with tarfile.open(fname, 'r') as f:\n",
    "        f.extractall(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:02<00:00, 5734.79it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:02<00:00, 5869.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:02<00:00, 5677.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 12500/12500 [00:02<00:00, 5773.18it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def read_imdb(folder='train', data_root=r\"D:\\CS\\MachineLearning\\Dive-into-DL-PyTorch-master/S1/CSCL/tangss/Datasets/aclImdb\"): \n",
    "    data = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder_name = os.path.join(data_root, folder, label)\n",
    "        for file in tqdm(os.listdir(folder_name)):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
    "                data.append([review, 1 if label == 'pos' else 0])\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "train_data, test_data = read_imdb('train'), read_imdb('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-Pre_processing_split words based with space\n",
    "def get_tokenized_imdb(data):\n",
    "    \"\"\"\n",
    "    data:list of [string,label]\n",
    "    \"\"\"\n",
    "    def tokenizer(text):\n",
    "        return [tok.lower() for tok in text.split(\" \")]\n",
    "    return [tokenizer(review) for review,_ in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('words in vocab :', 46152)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split words, filtering the words number<5\n",
    "def get_vocab_imdb(data):\n",
    "    tokenized_data=get_tokenized_imdb(data)\n",
    "    counter=collections.Counter([tk for st in tokenized_data for tk in st])\n",
    "    return Vocab.Vocab(counter,min_freq=5)\n",
    "\n",
    "vocab=get_vocab_imdb(train_data)\n",
    "\"words in vocab :\",len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fix the length to 500,using word index\n",
    "def preprocess_imdb(data,vocab):\n",
    "    max_l=500\n",
    "    \n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x)>max_l else x+[0]*(max_l-len(x))\n",
    "    \n",
    "    tokenized_data=get_tokenized_imdb(data)\n",
    "    features=torch.tensor([pad([vocab.stoi[word] for word in words]) for words in tokenized_data])\n",
    "    labels=torch.tensor([score for _,score in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estabilish iterator\n",
    "batch_size=64\n",
    "train_set=Data.TensorDataset(*preprocess_imdb(train_data,vocab))\n",
    "test_set=Data.TensorDataset(*preprocess_imdb(test_data,vocab))\n",
    "\n",
    "train_iter=Data.DataLoader(train_set,batch_size,shuffle=True)\n",
    "test_iter=Data.DataLoader(test_set,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([64, 500]) y torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('#batches', 391)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for X,y in train_iter:\n",
    "    print(\"X\",X.shape,\"y\",y.shape)\n",
    "    break\n",
    "\"#batches\",len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bulid BiRNN\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self,vocab,embed_size,num_hiddens,num_layers):\n",
    "        super(BiRNN,self).__init__()\n",
    "        self.embedding=nn.Embedding(len(vocab),embed_size)\n",
    "        #bidirectional 设为True\n",
    "        self.encoder=nn.LSTM(input_size=embed_size,\n",
    "                            hidden_size=num_hiddens,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=True)\n",
    "        #初始时间步和最终时间步的隐藏状态作为全连接层的输入\n",
    "        self.decoder=nn.Linear(4*num_hiddens,2)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        #inputs 的形状是（批量大小，词数），因为LSTM需要将序列长度（seq_len)作为第一维，所以将输入转置后\n",
    "        #在提取词特证，输出形状为（词数，批量大小，词向量维度）\n",
    "        embedding=self.embedding(inputs.permute(1,0))\n",
    "        ##rnn.LSTM只传入输入embeddings,因此只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        #output形状是（词数，批量大小，2*隐藏单元个数）\n",
    "        outputs,_=self.encoder(embedding) #output,(h,c)\n",
    "        #连接初始时间步和最终时间步的隐藏状态作为全连接层输入，他的形状为\n",
    "        #(批量大小，4*隐藏单元个数)\n",
    "        encoding=torch.cat((outputs[0],outputs[-1]),-1)\n",
    "        outs=self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialized BIRNN\n",
    "embed_size,num_hiddens,num_layers=100,100,2\n",
    "net=BiRNN(vocab,embed_size,num_hiddens,num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\CS\\MachineLearning\\Dive-into-DL-PyTorch-master\\S1\\CSCL\\tangss\\Datasets\\glove\\glove.6B.zip: 862MB [07:41, 1.87MB/s]  \n",
      "100%|██████████████████████████████████████████████████████████████████████▊| 399127/400000 [00:27<00:00, 14320.16it/s]"
     ]
    }
   ],
   "source": [
    "#load glove word vector ，increase dimension，avoid over-fitting\n",
    "glove_vocab=Vocab.GloVe(name=\"6B\",dim=100,cache=os.path.join(DATA_ROOT,\"glove\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words,pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取words的词向量\"\"\"\n",
    "    embed=torch.zeros(len(words),pretrained_vocab.vectors[0].shape[0])\n",
    "    oov_count=0#out of vocabulary\n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            idx=pretrained_vocab.stoi[word]\n",
    "            embed[i,:]=pretrained_vocab.vectors[idx]\n",
    "        except KeyError:\n",
    "            oov_count+=1\n",
    "    if oov_count>0:\n",
    "        print(\"There are %d oov words:\" % oov_count)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21202 oov words:\n"
     ]
    }
   ],
   "source": [
    "net.embedding.weight.data.copy_(\n",
    "    load_pretrained_embedding(vocab.itos,glove_vocab))\n",
    "net.embedding.weight.required_grad=False#直接加载训练好的，因此这里置false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "epoch 1, loss 0.5560, train acc 0.708, test acc 0.840, time 2559.9 sec\n",
      "epoch 2, loss 0.1383, train acc 0.889, test acc 0.831, time 2514.1 sec\n",
      "epoch 3, loss 0.0530, train acc 0.943, test acc 0.858, time 2506.0 sec\n",
      "epoch 4, loss 0.0238, train acc 0.968, test acc 0.859, time 2616.5 sec\n",
      "epoch 5, loss 0.0128, train acc 0.980, test acc 0.849, time 2618.3 sec\n"
     ]
    }
   ],
   "source": [
    "lr,num_epoches=0.01,5\n",
    "#过滤掉不计算梯度的embedding参数\n",
    "optimizer=torch.optim.Adam(filter(lambda p:p.requires_grad,net.parameters()),lr=lr)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "d2l.train(train_iter,test_iter,net,loss,optimizer,device,num_epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本函数已保存在d2lzh_pytorch包中方便以后使用\n",
    "def predict_sentiment(net, vocab, sentence):\n",
    "    \"\"\"sentence是词语的列表\"\"\"\n",
    "    device = list(net.parameters())[0].device\n",
    "    sentence = torch.tensor([vocab.stoi[word] for word in sentence], device=device)\n",
    "    label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "    return 'positive' if label.item() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'great']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'is', 'so', 'bad']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, vocab, ['this', 'movie', 'does',\"not\",\"contain\",\"actions\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
